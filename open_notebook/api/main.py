from typing import Optional

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from langchain_core.runnables import RunnableConfig
from loguru import logger

# Import the graph and initialize model_manager
from open_notebook.graphs.ask import graph
import open_notebook.model_manager # Ensures model_manager is initialized

# 3. Define a Pydantic model AskRequest
class AskRequest(BaseModel):
    question: str = Field(..., description="The question to ask the notebook.")
    strategy_model_id: Optional[str] = Field(None, description="Optional ID of the strategy model to use.")
    answer_model_id: Optional[str] = Field(None, description="Optional ID of the answer model to use.")
    final_answer_model_id: Optional[str] = Field(None, description="Optional ID of the final answer model to use.")

# Define the AskResponse Pydantic model
class AskResponse(BaseModel):
    final_answer: str = Field(..., description="The final answer generated by the notebook.")
    # sources_consulted: Optional[list[str]] = None # Optional, can be added later

# 4. Create a FastAPI app instance
app = FastAPI(
    title="Open Notebook API",
    description="API for interacting with Open Notebook functionalities, primarily for asking questions and getting answers processed by a series of language models."
)

# 5. Define a POST endpoint @app.post("/api/v1/ask")
@app.post(
    "/api/v1/ask",
    response_model=AskResponse,
    summary="Ask a question to the notebook",
    description="Receives a question and optional model configurations, processes it through the ask graph, and returns a final answer."
)
async def ask_question(request: AskRequest):
    graph_input = {"question": request.question}
    
    configurable = {}
    if request.strategy_model_id:
        configurable["strategy_model"] = request.strategy_model_id
    if request.answer_model_id:
        configurable["answer_model"] = request.answer_model_id
    if request.final_answer_model_id:
        configurable["final_answer_model"] = request.final_answer_model_id
        
    runnable_config = RunnableConfig(configurable=configurable)
    
    try:
        result = await graph.ainvoke(graph_input, config=runnable_config)
        
        # Validate result structure and extract final_answer
        if isinstance(result, dict) and "final_answer" in result and isinstance(result["final_answer"], str):
            response_data = AskResponse(final_answer=result["final_answer"])
            return response_data
        else:
            raise HTTPException(status_code=500, detail="Invalid response structure from ask graph")
            
    except Exception as e:
        # Basic error handling
        # If it's already an HTTPException, re-raise it, otherwise wrap it
        if isinstance(e, HTTPException):
            raise e
        logger.error(f"An unexpected error occurred during ask_question: {e}")
        raise HTTPException(status_code=500, detail=str(e))
