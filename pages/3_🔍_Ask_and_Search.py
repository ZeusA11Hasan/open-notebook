import asyncio

import streamlit as st

from open_notebook.domain.models import DefaultModels, model_manager
from open_notebook.domain.notebook import Note, Notebook, text_search, vector_search
from open_notebook.graphs.ask import graph as ask_graph
from pages.components.model_selector import model_selector
from pages.stream_app.utils import convert_source_references, setup_page, hide_header_and_padding

setup_page("T√¨m ki·∫øm", icon="üîç")

hide_header_and_padding()

ask_tab, search_tab = st.tabs(["ƒê·∫∑t c√¢u h·ªèi v·ªõi c∆° s·ªü tri th·ª©c (th·ª≠ nghi·ªám)", "T√¨m ki·∫øm"])

if "search_results" not in st.session_state:
    st.session_state["search_results"] = []

if "ask_results" not in st.session_state:
    st.session_state["ask_results"] = {}


async def process_ask_query(question, strategy_model, answer_model, final_answer_model):
    async for chunk in ask_graph.astream(
        input=dict(
            question=question,
        ),
        config=dict(
            configurable=dict(
                strategy_model=strategy_model.id,
                answer_model=answer_model.id,
                final_answer_model=final_answer_model.id,
            )
        ),
        stream_mode="updates",
    ):
        yield (chunk)


def results_card(item):
    score = item.get("relevance", item.get("similarity", item.get("score", 0)))
    with st.container(border=True):
        st.markdown(
            f"[{score:.2f}] **[{item['title']}](/?object_id={item['parent_id']})**"
        )
        if "matches" in item:
            with st.expander("Matches"):
                for match in item["matches"]:
                    st.markdown(match)


with ask_tab:
    st.subheader("ƒê·∫∑t c√¢u h·ªèi v·ªõi c∆° s·ªü tri th·ª©c (th·ª≠ nghi·ªám)")
    st.caption(
        "M√¥ h√¨nh LLM s·∫Ω tr·∫£ l·ªùi c√¢u h·ªèi c·ªßa b·∫°n d·ª±a tr√™n c√°c t√†i li·ªáu trong c∆° s·ªü tri th·ª©c c·ªßa b·∫°n. "
    )
    question = st.text_input("C√¢u h·ªèi", "")
    default_model = DefaultModels().default_chat_model
    strategy_model = model_selector(
        "M√¥ h√¨nh Chi·∫øn l∆∞·ª£c Truy v·∫•n",
        "strategy_model",
        selected_id=default_model,
        model_type="language",
        help="M√¥ h√¨nh LLM s·∫Ω x·ª≠ l√Ω c√°c truy v·∫•n chi·∫øn l∆∞·ª£c",
    )
    answer_model = model_selector(
        "M√¥ h√¨nh Tr·∫£ l·ªùi C√° nh√¢n",
        "answer_model",
        model_type="language",
        selected_id=default_model,
        help="M√¥ h√¨nh LLM s·∫Ω x·ª≠ l√Ω c√°c truy v·∫•n c√° nh√¢n",
    )
    final_answer_model = model_selector(
        "M√¥ h√¨nh Tr·∫£ l·ªùi Cu·ªëi c√πng",
        "final_answer_model",
        model_type="language",
        selected_id=default_model,
        help="M√¥ h√¨nh LLM s·∫Ω x·ª≠ l√Ω c√¢u tr·∫£ l·ªùi cu·ªëi c√πng",
    )
    if not model_manager.embedding_model:
        st.warning(
            "You can't use this feature because you have no embedding model selected. Please set one up in the Models page."
        )
    ask_bt = st.button("H·ªèi") if model_manager.embedding_model else None
    placeholder = st.container()

    async def stream_results():
        async for chunk in process_ask_query(
            question, strategy_model, answer_model, final_answer_model
        ):
            if "agent" in chunk:
                with placeholder.expander(
                    f"Agent Strategy: {chunk['agent']['strategy'].reasoning}"
                ):
                    for search in chunk["agent"]["strategy"].searches:
                        st.markdown(f"T√¨m ki·∫øm: **{search.term}**")
                        st.markdown(f"Y√™u c·∫ßu: {search.instructions}")
            elif "provide_answer" in chunk:
                for answer in chunk["provide_answer"]["answers"]:
                    with placeholder.expander("Tr·∫£ l·ªùi"):
                        st.markdown(convert_source_references(answer))
            elif "write_final_answer" in chunk:
                st.session_state["ask_results"]["answer"] = chunk["write_final_answer"][
                    "final_answer"
                ]
                with placeholder.container(border=True):
                    st.markdown(
                        convert_source_references(
                            chunk["write_final_answer"]["final_answer"]
                        )
                    )

    if ask_bt:
        placeholder.write(f"C√¢u h·ªèi: {question}")
        st.session_state["ask_results"]["question"] = question
        st.session_state["ask_results"]["answer"] = None

        asyncio.run(stream_results())

    if st.session_state["ask_results"].get("answer"):
        with st.container(border=True):
            with st.form("save_note_form"):
                notebook = st.selectbox(
                    "Notebook", Notebook.get_all(), format_func=lambda x: x.name
                )
                if st.form_submit_button("Th√™m c√¢u tr·∫£ l·ªùi v√†o ghi ch√∫"):
                    note = Note(
                        title=st.session_state["ask_results"]["question"],
                        content=st.session_state["ask_results"]["answer"],
                    )
                    note.save()
                    note.add_to_notebook(notebook.id)
                    st.success("T·∫°o th√†nh c√¥ng!")


with search_tab:
    with st.container(border=True):
        st.subheader("üîç T√¨m ki·∫øm")
        st.caption("T√¨m ki·∫øm trong c∆° s·ªü tri th·ª©c c·ªßa b·∫°n cho c√°c t·ª´ kh√≥a ho·∫∑c kh√°i ni·ªám c·ª• th·ªÉ")
        search_term = st.text_input("T√¨m ki·∫øm", "")
        if not model_manager.embedding_model:
            st.warning(
                "You can't use vector search because you have no embedding model selected. Only text search will be available."
            )
            search_type = "Text Search"
        else:
            search_type = st.radio("Ki·ªÉu t√¨m ki·∫øm", ["VƒÉn b·∫£n", "Vector"])
        search_sources = st.checkbox("T√¨m trong ngu·ªìn t√†i li·ªáu", value=True)
        search_notes = st.checkbox("T√¨m trong ghi ch√∫", value=True)
        if st.button("T√¨m"):
            if search_type == "VƒÉn b·∫£n":
                st.write(f"T√¨m ki·∫øm: {search_term}")
                st.session_state["search_results"] = text_search(
                    search_term, 100, search_sources, search_notes
                )
            elif search_type == "Vector":
                st.write(f"T√¨m ki·∫øm: {search_term}")
                st.session_state["search_results"] = vector_search(
                    search_term, 100, search_sources, search_notes
                )
        for item in st.session_state["search_results"]:
            results_card(item)
